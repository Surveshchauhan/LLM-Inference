{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "intro-header"
   },
   "source": [
    "# UdaciHeadline: LLM Inference Optimization Project\n",
    "\n",
    "## Project Introduction\n",
    "Large Language Models (LLMs) are transforming content creation, but deploying them efficiently remains a major hurdle. Imagine you're an ML Engineer at a bustling online news portal. Your key task? Automatically generating catchy headlines from article summaries using an LLM. The problem? The current inference process is sluggish, causing publication delays and driving up operational costs. In this project, UdaciHeadline, you'll step into this role and tackle this critical challenge head-on. Your mission is to accelerate the headline generation pipeline significantly by applying state-of-the-art LLM inference optimization techniques. Get ready to dive deep into practical optimization and deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary-header"
   },
   "source": [
    "## Project Summary\n",
    "This project provides hands-on experience in optimizing the inference performance of a pre-trained Large Language Model (like Llama-3.2-1B) for news headline generation. You will bring together concepts of LLM architecture, optimization techniques, and deployment frameworks. Specifically, you will:\n",
    "\n",
    "1.  **Establish a baseline** inference pipeline and profile its performance.\n",
    "2.  Implement and evaluate architectural optimizations like **KV-caching**.\n",
    "3.  Apply model compression techniques like **quantization** and **pruning**.\n",
    "4.  Configure and benchmark **distributed inference** using Tensor and Pipeline Parallelism.\n",
    "5.  Apply advanced decoding mechanisms like **speculative decoding**.\n",
    "6.  Perform comprehensive **benchmarking and analysis** across all stages.\n",
    "7.  Produce a **final report** summarizing findings and trade-offs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports-header"
   },
   "source": [
    "## Imports and Global Configuration\n",
    "\n",
    "Let's import the libraries we'll use throughout the project and define some constants like the model name and the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "imports-code"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from evaluate import load as load_metric\n",
    "from time import time\n",
    "from pprint import pprint\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\n",
    "# ---- Constants ----\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-1B\"\n",
    "MAX_NEW_TOKENS = 0 # Max length for the generated headline\n",
    "\n",
    "PROMPT = \\\n",
    "\"\"\"\n",
    "TODO: iterate on prompts to ensure good quality output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data-loading-header"
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "We will use the \"News Category Dataset\" from Kaggle. The `kagglehub` library makes it easy to download and access. Your task is to implement the function to load and preprocess the data according to the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;32mNews_Category_Dataset.json\u001b[0m*\n"
     ]
    }
   ],
   "source": [
    "ls ../../code/dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "data-loading-code"
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_news_dataset(path):\n",
    "    \"\"\"TODO: Implement the data loading and preprocessing logic here.\"\"\"\n",
    "    dataset = load_dataset(\"json\", data_files=path, split=\"train[:1000]\")\n",
    "    articles = [item[\"short_description\"] for item in dataset]\n",
    "\n",
    "    return dataset,articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baseline-header"
   },
   "source": [
    "# 2. Baseline Performance\n",
    "\n",
    "Before we can optimize, we need a starting point. Here, you'll establish the baseline performance of the `Llama-3.2-1B` model without any specific optimizations. We will measure latency, throughput, and the quality of the generated headlines using the ROUGE score.\n",
    "\n",
    "### Your Task: Implement the Evaluation Pipeline\n",
    "You need to implement the core functions for loading a model, generating a headline, and evaluating performance. These functions will be reused for every optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "baseline-helpers"
   },
   "outputs": [],
   "source": [
    "def load_model(model_name, quantization_config=None):\n",
    "    \"\"\"TODO: Implement the logic for loading a tokenizer and model.\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def generate_headline(model, tokenizer, summary, generation_args):\n",
    "    \"\"\"TODO: Implement the headline generation and latency measurement logic.\"\"\"\n",
    "    eadlines = []\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(\"cuda\")\n",
    "            start = time.time()\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                use_cache=False,  # Baseline: no KV caching\n",
    "                do_sample=False\n",
    "            )\n",
    "            end = time.time()\n",
    "            latencies.append(end - start)\n",
    "            headline = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            headlines.append(headline)\n",
    "    return headlines, latencies\n",
    "\n",
    "def report_metrics(times):#results, latencies, max_new_tokens):\n",
    "    \"\"\"TODO: Implement the logic for calculating and reporting all performance metrics.\"\"\"\n",
    "    avg_latency = sum(times) / len(times)\n",
    "    p99_latency = sorted(times)[int(0.99 * len(times))]\n",
    "    throughput = len(times) / sum(times)\n",
    "    gpu_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "    print(f\"Avg Latency: {avg_latency:.3f}s\")\n",
    "    print(f\"P99 Latency: {p99_latency:.3f}s\")\n",
    "    print(f\"Throughput: {throughput:.2f} samples/sec\")\n",
    "    print(f\"Max GPU Memory: {gpu_memory:.2f} MB\")\n",
    "\n",
    "    pass\n",
    "\n",
    "def evaluate_model(dataset, model, tokenizer, generation_args, n=20):\n",
    "    \"\"\"TODO: Implement the model evaluation loop.\"\"\"\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    references = [item[\"headline\"] for item in dataset[:100]]\n",
    "    results = rouge.compute(predictions=generated, references=references)\n",
    "    print(\"ROUGE Scores:\", results)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mLlama-3.2-1B\u001b[0m/  \u001b[01;34mLlama-3.3-70B-Instruct\u001b[0m/\n",
      "\u001b[01;34mLlama-3.2-3B\u001b[0m/  \u001b[01;34mLlama-4-Scout-17B-16E-Instruct\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /voc/shared/models/llama/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "baseline-eval"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver initialization failed, you might not have a CUDA gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: Establish your baseline performance\u001b[39;00m\n\u001b[1;32m      2\u001b[0m datasets, articles \u001b[38;5;241m=\u001b[39m load_news_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews_Category_Dataset.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/voc/shared/models/llama/Llama-3.2-1B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m sample_texts \u001b[38;5;241m=\u001b[39m articles[:\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m      5\u001b[0m generated, times \u001b[38;5;241m=\u001b[39m generate_headlines(sample_texts)\n",
      "Cell \u001b[0;32mIn[70], line 4\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_name, quantization_config)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"TODO: Implement the logic for loading a tokenizer and model.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2918\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` quantized models with the installed version of bitsandbytes. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2915\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current device is `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`. If you intended to move the model, please install bitsandbytes >= 0.43.2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2916\u001b[0m         )\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1050\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1050\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA driver initialization failed, you might not have a CUDA gpu."
     ]
    }
   ],
   "source": [
    "# TODO: Establish your baseline performance\n",
    "datasets, articles = load_news_dataset(\"News_Category_Dataset.json\")\n",
    "model = load_model(\"/voc/shared/models/llama/Llama-3.2-1B\")\n",
    "sample_texts = articles[:100]\n",
    "generated, times = generate_headlines(sample_texts)\n",
    "report_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kv-cache-header"
   },
   "source": [
    "# 3. Architectural Optimization: KV Caching\n",
    "\n",
    "**Your Task:** One of the most effective ways to speed up token generation is using a Key-Value (KV) cache. This avoids re-computing attention scores for tokens that are already part of the sequence. Enable the `use_cache` flag in the generation arguments and re-run the evaluation. Observe the impact on latency and throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kv-cache-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate the model with KV Caching enabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pruning-header"
   },
   "source": [
    "# 4. Model Compression: Pruning\n",
    "\n",
    "**Your Task:** Pruning removes redundant model weights, which can reduce model size and potentially speed up inference. Here, you will implement unstructured, magnitude-based pruning by creating a function that applies it to the model's linear layers and then evaluating the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pruning-code"
   },
   "outputs": [],
   "source": [
    "def prune_model_weights(model, amount=0.3):\n",
    "    \"\"\"TODO: Applies L1 unstructured pruning to the linear layers of a model.\"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO: Evaluate the pruned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quantization-header"
   },
   "source": [
    "# 5. Model Compression: Quantization\n",
    "\n",
    "**Your Task:** Quantization reduces the precision of model weights (e.g., from 16-bit to 4-bit), significantly cutting down memory usage and often speeding up inference. You will define a 4-bit quantization configuration and use it to load and evaluate a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quantization-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement and evaluate 4-bit quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "distributed-header"
   },
   "source": [
    "# 6. Distributed Inference (Multi-GPU)\n",
    "\n",
    "**Your Task:** If you have multiple GPUs, you can split the model across them to reduce the memory burden on a single GPU and potentially improve latency. We will explore two common techniques: Tensor Parallelism and Pipeline Parallelism.\n",
    "\n",
    "*Note: This section requires a multi-GPU environment.*\n",
    "\n",
    "### Tensor Parallelism\n",
    "Tensor parallelism splits individual model layers (the tensors) across multiple GPUs. Operations like matrix multiplications are executed in parallel on different GPUs, and the results are aggregated. This is highly effective for reducing the memory footprint of very large layers. The `accelerate` library can handle this automatically via `device_map=\"auto\"`.\n",
    "\n",
    "### Pipeline Parallelism\n",
    "Pipeline parallelism assigns entire layers or blocks of layers to different GPUs, creating a sequence or \"pipeline\" that the data flows through. For example, layers 1-10 run on GPU 0, layers 11-20 run on GPU 1, and so on. This is useful for very deep models where even a single layer might be too large for one GPU after tensor parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "distributed-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Check for multi-GPU environment and evaluate with Tensor Parallelism.\n",
    "# The `device_map=\"auto\"` in your `load_model` function should automatically apply this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pipeline-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluate with Pipeline Parallelism.\n",
    "# This is more advanced and may require manually defining a device_map to assign\n",
    "# different layers of the model to different GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "speculative-header"
   },
   "source": [
    "# 7. Advanced Decoding: Speculative Decoding\n",
    "\n",
    "**Your Task:** Speculative decoding uses a smaller, faster \"draft\" model to generate several candidate tokens. A larger, more accurate \"target\" model then verifies these tokens in a single forward pass. This can significantly speed up generation if the draft model is a good predictor. You will load a larger target model and a smaller draft model, benchmark the target model alone, and then benchmark it with assistance from the draft model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "speculative-code"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement and evaluate speculative decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "report-header"
   },
   "source": [
    "# 8. Final Report and Analysis\n",
    "\n",
    "**Your Task:** Consolidate your findings into a summary report. \n",
    "\n",
    "1.  Fill in the Markdown table below with the **Latency**, **Throughput**, and **ROUGE scores** for each optimization technique you implemented.\n",
    "2. Compile the final Project Report in PDF format:\n",
    "    *   Document the entire process, detailing the methodology, techniques, and libraries used.\n",
    "    *   Present the final benchmark results clearly.\n",
    "    *   Provide a thorough analysis of the trade-offs between performance, resources, and quality for each optimization step.\n",
    "    *   Conclude with recommendations for the most effective optimization strategy for this specific headline generation task, supported by your data.\n",
    "\n",
    "Some example questions for discussing the trade-offs:\n",
    "    *   Which method gave the best performance improvement?\n",
    "    *   Did any methods significantly hurt the ROUGE score (quality)?\n",
    "    *   Which optimization would you recommend for deployment in a production environment at the news portal, and why? Consider factors like cost, complexity, and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "report-table"
   },
   "source": [
    "## Performance Comparison\n",
    "\n",
    "| Optimization Technique | Mean Latency (s) | Throughput (tokens/s) | ROUGE-1 Score |\n",
    "|--------------------------|------------------|-----------------------|---------------|\n",
    "| Baseline (No Cache)      | TODO             | TODO                  | TODO          |\n",
    "| KV Caching               | TODO             | TODO                  | TODO          |\n",
    "| Pruning (30%)            | TODO             | TODO                  | TODO          |\n",
    "| Quantization (4-bit)     | TODO             | TODO                  | TODO          |\n",
    "| Tensor Parallelism       | TODO             | TODO                  | TODO          |\n",
    "| Pipeline Parallelism     | TODO             | TODO                  | TODO          |\n",
    "| Speculative Decoding     | TODO             | TODO                  | TODO          |\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
